---
title: "Final Project Data Analysis for Business -"
output:
  pdf_document: default
  html_document: default
---

```{r message=FALSE, warning=FALSE, include=FALSE}
# Importing libraries

library(ggplot2)
library(dplyr)
library(skimr)
library(readr)
library(sf)
library(usmap)
library(grid)
library(gridExtra)
library(corrplot)
library(caret)
library(ROSE)
library(latex2exp)
library(glmnet)
library(rpart)
library(rpart.plot)
library(randomForest)
library(doParallel)
library(pROC)
library(knitr)
library(xgboost)

```

# STATBIZ

Group members:

-   Michele Turco (285251)
-   Giulio Presaghi
-   Edoardo Brown
-   Irene Benvenuti

## GOAL OF THE ANALYSIS

## DATASET DESCRIPTION

### Importing the dataset

The dataset consists of 3333 observations with 20 variables related to telecom customer attributes, such as state, account length, service usage details, and whether the customer churned or not. In particular, we can see that there are 15 numerical variables and 5 categorical variables.

```{r echo=FALSE, warning=FALSE}
# Importing Dataset
Data = read.csv2("./Dataset/TelecomChurn.csv", 
                 header = T, 
                 sep = ",", 
                 colClasses = "character")
variables = colnames(Data)
categorical_variables = c("State", "International.plan", "Voice.mail.plan", "Area.code")
target_variable = "Churn"
numerical_variables = setdiff(variables, c(categorical_variables, target_variable))
predictors = setdiff(variables, target_variable)

# Ensuring that variables are converted in the correct form
Data[[target_variable]] = as.factor(Data[[target_variable]])
for (var in numerical_variables) {
  Data[[var]] = as.numeric(Data[[var]])
}
for (var in categorical_variables) {
  Data[[var]] = as.factor(Data[[var]])
}

# Showing the results
str(Data)
```

### Cleaning the dataset

The dataset does not contain any missing value and hence this is not an issue that need to be addressed.

```{r echo=FALSE}

#Renaming missing values with NA notation, and counting how many rows contain missing values, then printing result
na_counts_per_row = rowSums(is.na(Data))
rows_with_na = sum(na_counts_per_row > 0)
cat("Rows with NA before preprocessing:", rows_with_na, "\n")

```

In addition, the dataset does not present any duplicated value and consequently the issue does not need to be addressed.

```{r echo=FALSE}
# Counting duplicates row and printing the result
duplicates = sum(duplicated(Data))
print(paste("There are", duplicates, "duplicates rows in the Dataset"))
```

## EDA

Exploratory Data Analysis (EDA) is a crucial step in understanding the underlying patterns and relationships within the dataset. Through this analysis, we aim to gain insights that will inform the subsequent steps of data preprocessing and model building.

### Target variable distribution

We first consider our target variable "Churn" and its distribution. To do this, we will plot both a histogram to visualize the number of observations for each class and a pie chart to better understand their proportions in the dataset.

```{r include=FALSE}

# Computing churn count and proportion
churn_distribution = Data %>%
  count(Churn, name = "Count")
churn_distribution$proportion = churn_distribution$Count/sum(churn_distribution$Count)

# Creating histogram with ggplot library
histogram_plot = ggplot(Data, aes(x = Churn, fill = Churn)) + 
  geom_bar(color = "black", alpha = 0.7) +
  geom_text(stat='count', aes(label=after_stat(count)), vjust= 2) + 
  ggtitle("Churn Count") + 
  scale_fill_manual(values = c("red", "blue")) + 
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 12),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_text(size = 10),
    legend.position = "none",
  )

# Creating pie histogram plot with ggplot library
piechart_plot = ggplot(data = churn_distribution, aes(x = "", y = proportion, fill = Churn)) + 
  geom_bar(stat = "identity", color = "black", alpha = 0.7) +
  coord_polar("y") +
  theme_minimal() + 
  geom_text(aes(label = paste0(round(proportion * 100, 0), "%")), 
            position = position_stack(vjust = 0.5)) + 
  labs(x = NULL, y = NULL, fill = NULL, 
       title = paste("Churn Proportion")) + 
  scale_fill_manual(values = c("red", "blue", "green")) +
  theme(axis.line = element_blank(),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        plot.title = element_text(hjust = 0.5, size = 12),
        panel.grid = element_blank())

```

```{r echo=FALSE}

# Representing the plots

grid.arrange(
  arrangeGrob(histogram_plot, piechart_plot, nrow = 1, ncol = 2),
  top = textGrob("Target Variable Distribution", gp = gpar(fontsize = 20, fontface = "bold"))
)

```

The histogram on the left shows that there are 2850 customers who did not churn (False) and 483 customers who did churn (True). The pie chart on the right indicates that 86% of customers did not churn, while 14% did churn. This highlights an significance imbalance in the dataset, with an higher number of customers not churning compared to those who do.

### Categorical variables distribution

In order to get more insightful results in the analysis of the distribution of categorical variables, we firstly plot the proportion of each class in a pie chart (as we did before with our target variable).

```{r include=FALSE}
# Code used in order to generate the plots

plot_list = list()
plot_list_relationship = list()

# Loop through the categorical variables to create individual plots
for (variable in categorical_variables) {
  if (variable == "State") {
    next
  }
  
  # Creating histograms
  plot_relationship = ggplot(Data, aes_string(x = variable, fill = "Churn")) + 
    geom_bar(position = "dodge", color = "black", alpha = 0.7) + 
    scale_fill_manual(values = c("blue", "red")) + 
    geom_text(stat = 'count', aes(label = after_stat(count)), position = position_dodge(width = 1), vjust = -0.4) +
    xlab(variable) +
    ylim(0, 2700) +
    ylab("Count") + 
    theme_minimal() + 
    theme(
      plot.title = element_text(hjust = 0.5, size = 16),
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      axis.text.x = element_text(size = 10),
      axis.text.y = element_text(size = 10),
      legend.title = element_text(size = 14),
      legend.text = element_text(size = 12),
      legend.position = "none"
    )
  
  # Storing the legend separately for histograms (we want to represent only one in the final plot)
  get_legend <- function(myplot) {
    tmp <- ggplot_gtable(ggplot_build(myplot))
    leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
    legend <- tmp$grobs[[leg]]
    return(legend)
  }
  legend = get_legend(ggplot(Data, aes_string(x = categorical_variables[1], fill = "Churn")) + 
                         geom_bar(position = "dodge", color = "black", alpha = 0.7) + 
                         scale_fill_manual(values = c("blue", "red")) + 
                         theme_minimal() + 
                         theme(legend.title = element_text(size = 14),
                               legend.text = element_text(size = 12)))
  
  # Computing proportions for categorical variables
  count_df = Data %>%
    count(!!sym(variable), name = "Count")
  count_df$proportion = count_df$Count/sum(count_df$Count)
 
  # Creating pie charts
  plot = ggplot(data = count_df, aes(x = "", y = proportion, fill = !!sym(variable))) + 
    geom_bar(width = 1, stat = "identity", color = "black", alpha = 0.7) + 
    theme_classic() + 
    coord_polar("y") +
    geom_text(aes(label = paste0(round(proportion * 100, 0), "%")), 
              position = position_stack(vjust = 0.7)) + 
    labs(x = NULL, y = NULL, fill = NULL, 
         title = paste("Distribution of", variable)) + 
    guides(fill = guide_legend(reverse = TRUE)) + 
    scale_fill_manual(values = c("red", "blue", "green")) + 
    theme(axis.line = element_blank(),
          axis.text = element_blank(),
          axis.ticks = element_blank(),
          plot.title = element_text(hjust = 0.5))
  

  # Adding the plots to the list
  plot_list_relationship[[variable]] = plot_relationship
  plot_list[[variable]] = plot
}

```

```{r echo=FALSE}
# Display the first plot
grid.arrange(
  arrangeGrob(grobs = plot_list, nrow = 2, ncol = 2),
  top = textGrob("Categorical Variables Distribution", gp = gpar(fontsize = 20, fontface = "bold"))
)
```

The pie chart shows that 90% of the customers do not have an international plan, while only 10% have opted for one. This indicates that the majority of customers do not use international calling services. In addition, 72% of the customers do not have a voice mail plan, whereas 28% have subscribed to this service. This suggests that a significant portion of the customer base does not utilize voice mail services. The pie chart shows also the distribution of customers across three area codes: 408, 415, and 510. The distribution is relatively balanced, with 50% of customers having area code 415, and the remaining 50% evenly split between area codes 408 and 510 (25% each). This indicates that the customer base is fairly evenly distributed across these three regions.

Furthermore, we wanted to highlight the distribution of customer churn across the three categorical variables with an histogram in which the bars represent the counts of customers who churned (True) and did not churn (False) for each category.

```{r echo=FALSE}
grid.arrange(
  arrangeGrob(grobs = plot_list_relationship, nrow = 1, ncol = 3),
  top = textGrob("Churn Distribution Across Categorical Variables", gp = gpar(fontsize = 20, fontface = "bold"), just = "center"),
  right = legend,
  left = "Count"
)
```

The majority of customers without an international plan (2664) did not churn, whereas 346 customers did churn. For customers with an international plan, there are 186 non-churners compared to 137 churners.

Among customers without a voice mail plan, 2008 did not churn, while 403 churned. For customers with a voice mail plan, 842 did not churn and 80 churned.

The churn distribution is somewhat balanced across area codes, with 408 and 510 having slightly higher churn rates (122 and 125 churners respectively) compared to area code 415 (236 churners). The number of non-churners is highest in area code 415 (1419) compared to 408 (716) and 510 (715).

What is particularly interesting is that the proportion of churn is higher among customers with an international plan compared to those without. In fact, almost half of the customers who were subscribed to the International plan have churned, suggesting that this factor may heavily influence customer decisions. However, to draw any definitive conclusions, we need to perform a more in-depth analysis.

##### Chi-Square Test with International Plan and Churn

To statistically interpret the hypothesis stated above, we performed a Chi-Square Test on the two variables. The Chi-Square test is a statistical method used to determine if there is a significant association between two categorical variables. It compares the observed frequencies in each category to the expected frequencies under the null hypothesis, which posits that the variables are independent.

-   H0 (Null Hypothesis): There is no association between the two categorical variables; they are independent.

-   H1 (Alternative Hypothesis): There is an association between the two categorical variables; they are not independent.

```{r echo=FALSE}
# Create a contingency table and perform the chi-squared test. Then we report the results
contingency_table = table(Data$International.plan, Data$Churn)
chi_squared_test = chisq.test(contingency_table)
print("Chi Square Test on International Plan and Churn")
print(chi_squared_test)
```

X-squared = 222.57 is the test statistic value, which measures the discrepancy between the observed and expected frequencies. The extremely small p-value (\< 2.2e-16) indicates that the probability of observing such a discrepancy by random chance is very low. Since the p-value is much smaller than the conventional significance level (e.g., 0.05), the null hypothesis is rejected, suggesting that there is a statistically significant association between the two categorical variables in the dataset. In other words, the data provides strong evidence that the variables are not independent and that there is a significant relationship between them.

#### State variable

Due to its particularly high number of levels (51), we decided to analyze this variable differently from other categorical variables. A "classic" plot would not have been as insightful. Therefore, we plotted the churn rate across USA states directly on a map, (after ensuring rthat observations were balanced between states). This approach provides a clearer understanding of whether the variable distribution varies by state.

```{r echo=FALSE}
Data$ChurnNumeric = ifelse(Data$Churn == "True", 1, 0)

churn_rate_states = Data %>%
  group_by(State) %>%
  summarize(ChurnRate = mean(ChurnNumeric))

states = statepop
names(states)[names(states) == "abbr"] <- "State"
churn_rate_states = merge(states, churn_rate_states, by = "State", all.x = TRUE)

plot_usmap(data = churn_rate_states, values = "ChurnRate", labels = TRUE) +
  scale_fill_gradient(low = "lightblue",
                      high = "red",
                      name = NULL) +
  theme_void() +
  theme(legend.position = "bottom",
        legend.direction = "horizontal",
        legend.key.width = unit(0.8, "in"),
        plot.title = element_text(hjust = 0.5)) +
  ggtitle("Churn Rate Across US States")

# Dropping variables since it is now useless
Data$ChurnNumeric = NULL
```

The plot indicates that the churn rate distribution does vary significantly across states. For example, California (CA) and Texas (TX) exhibit higher churn rates, indicated by the deep red shading (25%, as shown in the legend), suggesting that a substantial proportion of customers in these states are leaving the service. In contrast, states like Alaska (AK) and Iowa (IA), shaded in blue, have much lower churn rates (\<10%), indicating better customer retention.

##### Chi squared test on State and Churn

We decided to perform the Chi-Square test in order to have a more accurate insight.

```{r echo=FALSE, warning=FALSE}
# Create a contingency table and perform the chi-squared test. Then we report the results
contingency_table = table(Data$State, Data$Churn)
chi_squared_test = chisq.test(contingency_table)
print("Chi Square Test on State and Churn")
print(chi_squared_test)
```

Since the p-value is less than 0.05 also in this case, we reject the null hypothesis (H0) that the two categorical variables are independent. This indicates a significant association between the variables. However, while both tests reject the null hypothesis, the first test shows a more pronounced discrepancy between observed and expected frequencies with a simpler model, while the second test shows a significant association in a more complex scenario, shown by the degrees of freedom df = 50 (higher number of categories for the state variable).

#### Numerical variables

In this section, we perform Exploratory Data Analysis (EDA) on numerical variables to understand their distributions, identify patterns, and detect any anomalies. This analysis will provide key insights and inform subsequent data preprocessing and modeling steps.

```{r include=FALSE, warning=FALSE}

# Code to generate the plots
Numerical_Data = Data[numerical_variables]
means_vec = apply(X = Numerical_Data, MARGIN = 2, FUN = mean)
median_vec = apply(X = Numerical_Data, MARGIN = 2, FUN = median)
sd_vec = apply(X = Numerical_Data, MARGIN = 2, FUN = sd)

plot_list_numerical_relationship = list()
plot_list_numerical = list()

# Loop through each numerical variable to create individual histograms
for (variable in numerical_variables) {
  # Setting appropriate bin width
  binwidth = ceiling(max(Data[[variable]], na.rm = TRUE)/10)
  # Removing points from the variables names
  name = gsub("\\.", " ", variable)
  
  # Creating histograms
  plot_numerical = ggplot(Data, aes_string(x = variable)) + 
    geom_histogram(binwidth = binwidth, color = "black", fill = "grey", alpha = 0.7) + 
    geom_vline(aes_string(xintercept = means_vec[variable]), color = "blue", linetype = "dashed", size = 1) + 
    geom_vline(aes_string(xintercept = median_vec[variable]), color = "red", linetype = "dashed", size = 1) + 
    labs(title = paste("Histogram of", variable), x = name, y = "Frequency") +
    theme_minimal() + 
    theme(
      plot.title = element_blank(),
      axis.title.x = element_text(size = 14),
      axis.title.y = element_blank(),
      axis.text.x = element_text(size = 12),
      axis.text.y = element_text(size = 12)
    )
  
  # Creating 
  plot_numerical_relationship = ggplot(Data, aes_string(x = variable, fill = "Churn")) +
    geom_histogram(position = "identity", alpha = 0.7, binwidth = binwidth) +
    labs(title = variable, x = name) +
    theme_minimal() +
    theme(
      plot.title = element_blank(),
      axis.title.x = element_text(size = 12),
      axis.title.y = element_blank(),
      axis.text.x = element_text(size = 10),
      axis.text.y = element_text(size = 10)
    ) +
    scale_fill_manual(name = "Churn", labels = c("No", "Yes"), values = c("blue", "red"))
  
  # Storing legend
  legend = get_legend(ggplot(Data, aes_string(x = variable, fill = "Churn")) +
                        geom_histogram(position = "identity", alpha = 0.7, binwidth = binwidth) +
                        labs(title = variable) +
                        theme_minimal() +
                        theme(legend.title = element_text(size = 14),
                              legend.text = element_text(size = 12),
                              legend.direction = "horizontal") +
                        scale_fill_manual(name = "Churn", labels = c("No", "Yes"), values = c("blue", "red"))
                      ) 
  
  # Add the plot to the list
  plot_list_numerical[[variable]] = plot_numerical
  plot_list_numerical_relationship[[variable]] = plot_numerical_relationship
}

```

We decided to follow a similar approach as the one with categorical variables. We firstly plotted histograms that showed the distribution of the data and then we considered their relationship with the target variable.

```{r fig.width=10, fig.height=10, echo=FALSE, warning=FALSE}

grid.arrange(
  arrangeGrob(grobs = plot_list_numerical, nrow = 5, ncol = 3),
  top = textGrob("Numerical Variables Distribution", gp = gpar(fontsize = 20, fontface = "bold"))
)

```

The histograms reveal that several variables, such as Account length, Total day minutes, and Total night charge, exhibit relatively symmetric distributions approximating normality. In contrast, variables like Number vmail messages and Customer service calls display right skewness, where most values are low with a few higher values. Notably, Number vmail messages and Total intl calls have significant zero values, suggesting under utilization of these services. The red dashed lines represent the means, indicating the central tendency for each variable.

```{r fig.width=10, fig.height=10, echo=FALSE, warning=FALSE}

grid.arrange(
  arrangeGrob(grobs = plot_list_numerical_relationship, nrow = 5, ncol = 3),
  top = textGrob("Numerical Variables Distribution", gp = gpar(fontsize = 20, fontface = "bold")),
  bottom = legend,
  left = textGrob("Count", rot = 90, gp = gpar(fontsize = 14))
)

```

Across most variables, the proportion of customers who churn (red) is relatively small compared to those who do not churn (blue), consistent with the overall churn rate in the dataset.

For Customer service calls, the churn proportion noticeably increases with higher values, suggesting that frequent contact with customer service may indicate dissatisfaction leading to churn.

In Total day minutes, customers with very high usage show a slightly higher churn rate, potentially due to issues related to usage limits or service quality.

Consequently, we decided to better explore the relationships between the target variable and these two categorical variables.

##### ANOVA test on Total Day Minutes and Churn

The ANOVA (Analysis of Variance) test between a categorical and a continuous variable is a statistical method used to determine if there are significant differences in the means of the continuous variable across the levels of the categorical variable. It tests the null hypothesis that the means are equal across all groups against the alternative hypothesis that at least one group mean is different.

In the first place, we better represent the data with boxplots.

```{r echo=FALSE}

ggplot(Data, aes(x = Churn, y = `Total.day.minutes`, fill = Churn)) + 
  geom_boxplot() + 
  ggtitle("Total Day Minutes by Churn Status") + 
  xlab("Churn") + 
  ylab("Total Day Minutes") +
  scale_fill_manual(values = c("red", "blue")) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16),
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
  )

```

Churners (blue) have a higher median Total Day Minutes compared to non-churners (red). The spread of Total Day Minutes is similar for both groups, but non-churners have more outliers with lower usage. This confirms that higher Total Day Minutes among churners suggest that heavy usage might be linked to a higher likelihood of churn, while the inactive users (outliers) do not churn at all.

```{r echo=FALSE, warning=FALSE}


# Perform an ANOVA to test the hypothesis H0 that the two means (of the groups),
# are the same, suggesting that having more minutes is not a valid indicator for Churn.

anova_total_minutes = aov(Total.day.minutes ~ Churn, data = Data)
print("ANOVA Test on Total day minutes and Churn")
summary(anova_total_minutes)
```

The ANOVA test shows a significant effect of Churn on the continuous variable analyzed, with a very high F value (146.4) and a p-value \< 2e-16. This indicates that the means of Total day minutes differ significantly between the churn and non-churn groups.

##### ANOVA test on Customer Service Calls and Churn

We repeat the same test for Customer Service Calls

```{r echo=FALSE, warning=FALSE}

anova_service_calls = aov(Customer.service.calls ~ Churn, data = Data)
print("ANOVA Test on Customer Service Calls and Churn")
summary(anova_service_calls)

```

In this case, the ANOVA results show a highly significant effect of the churn status on the continuous variable, with a very low p-value (\< 2e-16). This indicates a significant difference in the means of the continuous variable between customers who churn and those who do not. The high F value (151.8) suggests that the churn status explains a substantial portion of the variability in the continuous variable, more so than indicated by the previous ANOVA results.

### Correlation Analysis

In this section, we measure the correlation between numerical variables. Correlation is a statistical measure that describes the extent to which two variables change together. It is particularly relevant since collinearity, or multicollinearity, occurs when two or more predictor variables in a dataset are highly correlated. This means that these variables share similar information and move together in a predictable way. We will deal later with this aspect.

```{r fig.width=10, echo=FALSE}

### COLLINEARITY

cor_matrix = cor(Data[numerical_variables], use="complete.obs")
corrplot(cor_matrix, method = "color", tl.srt = 45, tl.col = "black",
         addCoef.col = "black", 
         number.cex = 0.7,
         addgrid.col = "grey",
         tl.cex = 0.8,
         col = colorRampPalette(c("red", "white", "blue"))(200))

```

This correlation matrix shows the relationships between various numerical variables in the dataset. It reveals that no variables are strongly correlated with each other. However, there are some variables that are essentially the same quantity, such as Total day minutes and Total day charge, Total eve minutes and Total eve charge, and Total night minutes and Total night charge, as indicated by their perfect or near-perfect correlations (correlation coefficients close to 1). This is because the total charge for each type is computed by multiplying the total minutes by a constant (or nearly constant) factor. For instance, in the case of day charge, the factor is 0.17. Hence, these pairs of variables provide the same information.

## Preprocessing

### Dealing with collinearity

As stated above, considering collinearity and correlations is crucial in classification problems because highly correlated variables can lead to redundancy and affect the model's performance. When predictors are highly correlated, they may provide overlapping information, which can complicate the model's ability to learn effectively and can also inflate the importance of certain features. By identifying and addressing these correlations, we can simplify the model, improve its interpretability, and enhance its predictive performance. Consequently, we proceed in removing variables that are highly correlated to others.

```{r echo=FALSE}

# We have seen in our EDA that there are more features with a really high correlation
# and hence we have to drop them.

cor_matrix[!lower.tri(cor_matrix)] = 0

# Find the pairs with correlation greater than the threshold
threshold = 0.8
high_corr_pairs = which(abs(cor_matrix) > threshold, arr.ind = TRUE)

# Create a data frame with the results
high_corr_df = data.frame(
  Variable1 = rownames(cor_matrix)[high_corr_pairs[, 1]],
  Variable2 = colnames(cor_matrix)[high_corr_pairs[, 2]],
  Correlation = cor_matrix[high_corr_pairs]
)

variables_to_drop = high_corr_df$Variable1

for (variable in variables_to_drop) {
  Data[[variable]] = NULL
  cat("Dropped", variable)
  cat("\n")
  numerical_variables = setdiff(numerical_variables, variable)
  variables = setdiff(variables, variable)
  predictors = setdiff(predictors, variable)
}

```

### Dealing with categorical variable with too many levels

Having a categorical variable with too many levels in the dataset can increase model complexity, lead to overfitting, reduce computational efficiency, and make the model harder to interpret. In our case, the variable State has too many levels and hence needs to be preprocessed to reduce its dimension. After some research, we found that since 1950, the United States Census Bureau (a principal agency of the U.S. Federal Statistical System, responsible for producing data about the American people) defines four statistical regions: Northeast, West, South, and Midwest. This division is widely used for data collection and analysis and fits our case particularly well as it reduces the levels of the variable from 51 to 4, a significant improvement.

```{r echo=FALSE}
# Create a list to store regions and their states
dic = list()
dic$Northeast = c("CT", "ME", "MA", "NH", "NJ", "NY", "PA", "RI", "VT")
dic$Midwest = c("IL", "IN", "IA", "KS", "MI", "MN", "MO", "NE", "ND", "OH", "SD", "WI")
dic$South = c("AL", "AR", "DE", "DC", "FL", "GA", "KY", "LA", "MD", "MS", "NC", "OK", "SC", "TN", "TX", "VA", "WV")
dic$West = c("AK", "AZ", "CA", "CO", "HI", "ID", "MT", "NV", "NM", "OR", "UT", "WA", "WY")


# Function to find the region for a state
find_region <- function(state) {
  for (region in names(dic)) {
    if (state %in% dic[[region]]) {
      return(region)
    }
  }
  return(NA)
}

# Apply the function to each state in the data frame to add the region column
Data$Region = sapply(Data$State, find_region)
print("Distinct values for the Region variable:")
# print(unique(Data$Region))

# Converting region in a factor and dropping (but storing) the State variable
Data$Region = as.factor(Data$Region)
states = Data$State
Data$State = NULL
variables = setdiff(variables, "State")
categorical_variables = setdiff(categorical_variables, "State")
predictors = setdiff(predictors, "State")
variables = c(variables, "Region")
categorical_variables = c(categorical_variables, "Region")
predictors = c(predictors, "Region")

```

## Splitting and Scaling the data

In the first place, we decided to split the dataset into a training set (75% of the data) and a test set (25% of the data). We examined the distribution of the target variable Churn for the original, training, and test sets, displaying the counts and proportions of churned vs. non-churned customers.

```{r echo=FALSE}
### NORMAL SPLIT

set.seed(1)

id_train = sample(1:nrow(Data), size = 0.75*nrow(Data), replace = F)
train_data = Data[id_train,]
test_data = Data[-id_train,]

# Response variable distribution in the original data
cat("Distribution of the target variable in the original set:\n")
cat("Counts:")
print(table(Data$Churn))
cat("Proportions:")
print(prop.table(table(Data$Churn)))
cat("\n")

# Response variable distribution in the train test
cat("Distribution of the target variable in the train set:\n")
cat("Counts:")
print(table(train_data$Churn))
cat("Proportions:")
print(prop.table(table(train_data$Churn)))
cat("\n")

# Response variable distribution in the test set
cat("Distribution of the target variable in the validation set:\n")
cat("Counts:")
print(table(test_data$Churn))
cat("Proportions:")
print(prop.table(table(test_data$Churn)))

```

To address the unbalanced dataset, we created a training set using oversampling. Oversampling is important as it provides the model with enough examples of the minority class, helping it learn the characteristics of both classes more accurately. This improves the model's ability to predict minority class instances, which is crucial in our analysis since predicting customer churn is a key objective for Telecom.

We also attempted undersampling, as suggested in the R reference script of the project. However, the results were quite poor due to the reduced number of data points available for training, which limited the model's learning capability. Consequently, we did not report our findings using this technique.

```{r echo=FALSE}

### OVERSAMPLING

set.seed(1)

# Calculate the total number of samples needed for balanced oversampling
# We want each class to have max_class_count samples
target_N <- 2 * max(table(train_data$Churn))

# Perform oversampling
train_data_oversample <- ovun.sample(Churn ~ ., data = train_data, method = "over", N = target_N)$data

# Response variable distribution in the train set with over sampling
cat("Distribution of the target variable in the oversampled test set:\n")
cat("Counts:")
print(table(train_data_oversample$Churn))
cat("Proportions:")
print(prop.table(table(train_data_oversample$Churn)))

```

Afterwards, we scaled the numerical variables in the training and oversampled datasets using their respective means and standard deviations. Scaling ensures that all numerical features contribute equally to the model’s learning process, preventing features with larger ranges from dominating the model’s behavior. We also scaled the test set using the means and standard deviations from the original training set, ensuring consistency in data preprocessing.

```{r include=FALSE, warning=FALSE}

### SCALING

cols_to_scale = numerical_variables

# Normal data
train_mean = apply(train_data[, cols_to_scale], MARGIN = 2, FUN = mean)
train_sd = apply(train_data[, cols_to_scale], MARGIN = 2, FUN = sd)
train_data_scaled = train_data
train_data_scaled[, cols_to_scale] = scale(train_data[, cols_to_scale], 
                                                      center = train_mean, 
                                                      scale = train_sd)

# Oversample data
train_oversample_mean = apply(train_data_oversample[, cols_to_scale], MARGIN = 2, FUN = mean)
train_overssample_sd = apply(train_data_oversample[, cols_to_scale], MARGIN = 2, FUN = sd)
train_data_scaled_oversample = train_data_oversample
train_data_scaled_oversample[, cols_to_scale] = scale(train_data_oversample[, cols_to_scale], 
                                                      center = train_oversample_mean, 
                                                      scale = train_overssample_sd)


# Scale validation data using training data's parameters
test_data_scaled = test_data
test_data_scaled[, cols_to_scale] = scale(test_data_scaled[, cols_to_scale], 
                                          center = train_mean, 
                                          scale = train_sd)

test_data_scaled_oversample = test_data
test_data_scaled_oversample[, cols_to_scale] = scale(test_data_scaled_oversample[, cols_to_scale], 
                                                     center = train_oversample_mean, 
                                                     scale = train_overssample_sd)

```

## BEST MODEL SELECTION

```{r include=FALSE}

#### Function to compare performances

get.metrics<- function(conf.mat) {
  true.positives <- conf.mat[2,2]
  true.negatives <- conf.mat[1,1]
  false.positives <- conf.mat[1,2]
  false.negatives <- conf.mat[2,1]
  num.observations <- true.positives + true.negatives + false.positives + false.negatives
  
  accuracy <- (true.positives + true.negatives) / num.observations
  precision <- (true.positives) / (true.positives + false.positives)
  recall <- true.positives / (true.positives + false.negatives)
  f1 <- 2 * ((precision * recall) / (precision + recall))
  
  metrics <- data.frame(t(c(accuracy, precision, recall, f1)))
  columns <- c("Accuracy", "Precision", "Recall", "F1")
  colnames(metrics) <- columns
  
  return(metrics)
}

# Create a list to store every confusion matrix

confusion_matrices = list()

```

```{r include=FALSE}
# Function to plot metrics
plot_table = function(df) {
  
  table_theme = ttheme_default(
    core = list(
      bg_params = list(fill = c(rep(c("white"), each = nrow(df)), NA), col = "black"),
      fg_params = list(fontface = 1, fontsize = 16)
    ),
    colhead = list(
      bg_params = list(fill = "lightblue", col = "black"),
      fg_params = list(fontface = 2, fontsize = 18)
    ),
    rowhead = list(
      bg_params = list(fill = "lightblue", col = "black"),
      fg_params = list(fontface = 2, fontsize = 18)
    )
  )
  table_plot = tableGrob(df, theme = table_theme)
  return(table_plot)
}

```

```{r include=FALSE}

# Function to plot confusion matrices 

plot_confusion_matrix <- function(conf_matrix, title) {
  conf_df <- as.data.frame(as.table(conf_matrix))
  colnames(conf_df) <- c("Predicted", "Actual", "Freq")
  
  ggplot(data = conf_df, aes(x = Predicted, y = Actual, fill = Freq)) +
    geom_tile(color = "black") +
    geom_text(aes(label = Freq), vjust = 1) +
    scale_fill_gradient(low = "white", high = "red") +
    labs(title = title, x = "Predicted", y = "Actual") +
    theme_minimal() +
    theme(plot.title = element_text(hjust = 0.5),
          legend.position = "none")
}

```


In this section, we explore various models and techniques in order to train the best possible model in order to predict consumer churn. Since in general the oversampled dataset provided better performances, we are going to consider only the latter, except for cases in which certain results are particularly relevant.

### Logistic Regression

In the first place, we consider as baseline the models in which no variable is used and all variables are included. Firstly, we build the model with all the variables.

```{r warning=FALSE}
set.seed(1)
model_baseline_logistic_oversample = glm(Churn ~ ., 
                              data = train_data_scaled_oversample, 
                              family = "binomial")

```


```{r include=FALSE}

# Storing the results in a confusion matrix
probabilities_baseline_logistic_oversample = predict(model_baseline_logistic_oversample, 
                                                          newdata = test_data_scaled_oversample,
                                                   type = "response")
predictions_baseline_logistic_oversample = ifelse(probabilities_baseline_logistic_oversample > 0.5, "True", "False")
confusion_matrix_baseline_logistic_oversample = table(predictions_baseline_logistic_oversample, 
                                                      test_data_scaled_oversample$Churn)

```

Then, we consider the model without any variable:

```{r}
model_0 = glm(Churn ~ 1,
                   family = "binomial",
                   data = train_data_scaled_oversample)

```

And subsequently we test the hypothesis of equivalence between the two models

```{r}

anova(model_0, model_baseline_logistic_oversample, test = "Chisq")

```
The null model fits the data poorly with a deviance of 5905.6. The full model, which includes all the specified predictors, fits the data significantly better, with a deviance of 4437.3. The reduction in deviance (1468.3) is highly significant, with a p-value < 2.2e-16. Including the predictors in Model 2 significantly improves the fit of the model, 
indicating that the predictors collectively provide valuable information in predicting customer churn.

### Variable selection with BIC and AIC

Variable selection is a crucial step in building efficient and interpretable models. It involves identifying the most relevant predictors from a set of potential variables. Two commonly used criteria for model selection are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC).

#### AIC

AIC is used to compare models, with a focus on finding the model that best balances fit and complexity. It penalizes the number of parameters, discouraging overfitting, while prioritizing models with a better fit to the data.
We performed feature selection using three different methods: forward selection, backward selection, and both directions selection.

```{r include=FALSE}
set.seed(1)
aic_model_forward_oversample = step(glm(Churn ~ 1, 
                      family = "binomial", 
                      data = test_data_scaled_oversample), 
                  scope = formula(model_baseline_logistic_oversample), 
                  direction = "forward")
aic_model_backward_oversample = step(model_baseline_logistic_oversample, direction = "backward")
aic_model_both_oversample = step(model_baseline_logistic_oversample, direction = "both")

# Number of features selected by each model
num_features_aic_forward_oversample = length(coef(aic_model_forward_oversample)) - 1
num_features_aic_backward_oversample = length(coef(aic_model_backward_oversample)) - 1
num_features_aic_both_oversample = length(coef(aic_model_both_oversample)) - 1

# Selected variables
selected_variables_aic_forward_oversample = names(coef(aic_model_forward_oversample))[-1]
selected_variables_aic_backward_oversample = names(coef(aic_model_backward_oversample))[-1]
selected_variables_aic_both_oversample = names(coef(aic_model_both_oversample))[-1]

```

```{r echo=FALSE}

# Print the number of features selected and the selected variables for each method
cat("With forward selection", num_features_aic_forward_oversample, "features have been selected.")
cat("\n")
print(paste(selected_variables_aic_forward_oversample, collapse = ", "))
cat("\n")

cat("With backward selection", num_features_aic_backward_oversample, "features have been selected")
cat("\n")
print(paste(selected_variables_aic_backward_oversample, collapse = ", ")) 
cat("\n")

cat("With both directions selection", num_features_aic_both_oversample, "features have been selected")
cat("\n")
print(paste(selected_variables_aic_both_oversample, collapse = ", ")) 
cat("\n")


```
Given the goal of creating a simpler model to avoid overfitting and enhance interpretability, we will choose the model generated by forward selection. This model includes only 9 features, providing a balance between simplicity and predictive power.

```{r include=FALSE}
# Select the simplest model, hence the one with the lowest features selected
feature_counts_oversample = c(forward = num_features_aic_forward_oversample, 
                              backward = num_features_aic_backward_oversample, 
                              both = num_features_aic_both_oversample)
selected_model_name_oversample = names(which.min(feature_counts_oversample))

if (selected_model_name_oversample == "forward") {
  model_aic_final_oversample = aic_model_forward_oversample
} else if (selected_model_name_oversample == "backward") {
  model_aic_final_oversample = aic_model_backward_oversample
} else {
  model_aic_final_oversample = aic_model_both_oversample
}

# Computing and storing predictions on the validation data
# Computing and storing predictions on the test data
probabilities_aic_oversample = predict(model_aic_final_oversample, test_data_scaled_oversample, type = "response")
predictions_aic_oversample = ifelse(probabilities_aic_oversample> 0.5, "True", "False")
confusion_matrix_aic_oversample = table(predictions_aic_oversample, test_data_scaled_oversample$Churn)

```

#### BIC

BIC is similar to AIC but applies a stronger penalty for the number of parameters. It is particularly useful for selecting simpler models, as it penalizes model complexity more heavily than AIC.

```{r include=FALSE}
set.seed(1)
bic_model_forward_oversample = step(glm(Churn ~ 1, 
                                        family = "binomial", 
                                        data = test_data_scaled_oversample), 
                                    scope = formula(model_baseline_logistic_oversample), 
                                    direction = "forward",
                                    k = log(nrow(test_data_scaled_oversample)))
bic_model_backward_oversample = step(model_baseline_logistic_oversample, direction = "backward", k = log(nrow(test_data_scaled_oversample)))
bic_model_both_oversample = step(model_baseline_logistic_oversample, direction = "both", k = log(nrow(test_data_scaled_oversample)))

# Number of features selected by each model
num_features_bic_forward_oversample = length(coef(bic_model_forward_oversample)) - 1
num_features_bic_backward_oversample = length(coef(bic_model_backward_oversample)) - 1
num_features_bic_both_oversample = length(coef(bic_model_both_oversample)) - 1

# Selected variables
selected_variables_bic_forward_oversample = names(coef(bic_model_forward_oversample))[-1]
selected_variables_bic_backward_oversample = names(coef(bic_model_backward_oversample))[-1]
selected_variables_bic_both_oversample = names(coef(bic_model_both_oversample))[-1]

```

```{r echo=FALSE}

# Print the number of features selected and the selected variables for each method
cat("With forward selection (BIC)", num_features_bic_forward_oversample, "features have been selected.")
cat("\n")
print(paste(selected_variables_bic_forward_oversample, collapse = ", "))
cat("\n")

cat("With backward selection (BIC)", num_features_bic_backward_oversample, "features have been selected")
cat("\n")
print(paste(selected_variables_bic_backward_oversample, collapse = ", "))
cat("\n")

cat("With both directions selection (BIC)", num_features_bic_both_oversample, "features have been selected")
cat("\n")
print(paste(selected_variables_bic_both_oversample, collapse = ", "))
cat("\n")

```
Again, the simplest model (the one with the lowest number of covariates) should be selected. In this case, with forward selection BIC selected 7 features confirming that it applies a stronger penalty for model complexity, often leading to simpler models.

In general, the process of feature selection confirmed that the variables International Plan, Total day minutes and Customer Service calls are relevant in order to predict customer Churn. Also, considering the stricter model we discover that the Region (State) categorical variable is not taken into account, while Voice mail plan, Total eve minutes, Total night minutes and Total intl minutes are kept in the model. The same features are selected by AIC that also considers Total intl calls and Total day calls.

```{r}

# Select the simplest model, hence the one with the lowest features selected
feature_counts_bic_oversample = c(forward = num_features_bic_forward_oversample, 
                                  backward = num_features_bic_backward_oversample, 
                                  both = num_features_bic_both_oversample)
selected_model_name_bic_oversample = names(which.min(feature_counts_bic_oversample))

if (selected_model_name_bic_oversample == "forward") {
  model_bic_final_oversample = bic_model_forward_oversample
} else if (selected_model_name_bic_oversample == "backward") {
  model_bic_final_oversample = bic_model_backward_oversample
} else {
  model_bic_final_oversample = bic_model_both_oversample
}

# Computing and storing predictions on the validation data
# Computing and storing predictions on the validation data
probabilities_bic_oversample = predict(model_bic_final_oversample, test_data_scaled_oversample, type = "response")
predictions_bic_oversample = ifelse(probabilities_bic_oversample > 0.5, "True", "False")
confusion_matrix_bic_oversample = table(predictions_bic_oversample, test_data_scaled_oversample$Churn)

```

### Comparisson Linear models

In the first place, we report the confusion matrix for each of the three models that were previously trained in order to have a general understanding of their performance. It is important to recall that each prediction has been made on the test set (on which the models were not trained) that was previously scaled with the ::: of the test set.

```{r echo=FALSE}

# Create the plots
baseline_plot = plot_confusion_matrix(confusion_matrix_baseline_logistic_oversample, "Baseline Model")
aic_plot = plot_confusion_matrix(confusion_matrix_aic_oversample, "AIC Model")
bic_plot = plot_confusion_matrix(confusion_matrix_bic_oversample, "BIC Model")

# Arrange the plots in a single layout
grid.arrange(baseline_plot, aic_plot, bic_plot, ncol = 3, top = textGrob("Confusion Matrices", gp = gpar(fontsize = 20, fontface = "bold")))

```
Also, we consider their performance in terms of AUC (Area Under ROC Curve).

```{r echo=FALSE, warning=FALSE}
# Compute ROC curves

library(pROC)
par(mfrow=c(1,3)) # 3 rows, 1 column layout for confusion matrices
roc_baseline = roc(test_data$Churn, 
                   predict(model_baseline_logistic_oversample, 
                           newdata = test_data_scaled_oversample, 
                           type = "response"), 
                   plot = TRUE, main = "Baseline Model", col = "blue", lwd = 3, 
                   auc.polygon = TRUE, print.auc = TRUE)
roc_aic = roc(test_data$Churn, 
                   predict(model_aic_final_oversample, 
                           newdata = test_data_scaled_oversample, 
                           type = "response"), 
                   plot = TRUE, main = "AIC Model", col = "red", lwd = 3, 
                   auc.polygon = TRUE, print.auc = TRUE)
roc_bic = roc(test_data$Churn, 
              predict(model_bic_final_oversample, 
                      newdata = test_data_scaled_oversample, 
                      type = "response"), 
              plot = TRUE, main = "BIC Model", col = "green", lwd = 3, 
              auc.polygon = TRUE, print.auc = TRUE)
```

### Penalized approaches

#### Lasso Regression

Lasso regression is a linear regression technique that includes a penalty term to reduce model complexity and prevent overfitting. It performs feature selection by shrinking some coefficients to zero, effectively selecting only the most important predictors. This helps in building simpler and more interpretable models.

```{r echo=FALSE, warning=FALSE}

set.seed(1)
# Train the Lasso regression model on the oversampled data
X_train_oversample = train_data_scaled_oversample
Y_train_oversample = as.numeric(X_train_oversample$Churn)
X_train_oversample$Churn = NULL
fit_oversample = glmnet(as.matrix(X_train_oversample), Y_train_oversample, alpha = 1, lambda = seq(0, 0.15, length = 30))

# Plot coefficient values against the (log-)lambda sequence
predictor_names_oversample = colnames(as.matrix(X_train_oversample))
colors_oversample = sample(1:length(predictor_names_oversample))
plot(fit_oversample, xvar = "lambda", label = TRUE, col = colors_oversample)
legend("topright", legend = predictor_names_oversample, col = colors_oversample, lty = 1, cex = 0.5, text.width = 1.2)

```

This plot shows how the coefficients of different features in a Lasso regression model change as the regularization parameter increases (left on the x-axis). As it increases, the coefficients shrink toward zero, demonstrating Lasso's feature selection capability. Features with coefficients that remain large even at higher values are considered to be more important, while those that quickly shrink to zero are less important. 

However, in order to train the model a crucial step is the hyperparameter tuning.

```{r}
set.seed(1)
# Train the Lasso regression model on the oversampled data with tuning
model_lasso_oversample = train(Churn ~ ., 
                               data = train_data_scaled_oversample, 
                               method = "glmnet", 
                               metric = "Accuracy", 
                               trControl = trainControl(method = "cv", number = 10), 
                               tuneGrid = expand.grid(alpha = 1, lambda = seq(0, 0.15, length = 30)))

# Plot accuracy values against the lambda sequence
plot(model_lasso_oversample,
     label = TRUE, 
     xvar = "lambda")

# Retrieve the maximum accuracy and best tuning parameters 
# achieved during cross-validation
best_accuracy_lasso_oversample = max(model_lasso_oversample$results$Accuracy)
best_parameters_lasso_oversample = model_lasso_oversample$bestTune$lambda
cat("The highest value of accuracy:", best_accuracy_lasso_oversample, "is obtained with lambda =", best_parameters_lasso_oversample, "\n")

```

The best accuracy is obtained without a regularization parameter: in follows that the model built is a simple linear regression model. As the parameter continues to increase beyond 0.05, the accuracy starts to decline more steeply, indicating that the model is becoming overly regularized. This excessive penalization of the coefficients leads to underfitting, where the model is too simple to capture the underlying patterns in the data.

```{r include=FALSE}

# Computing and storing predictions
probabilities_lasso_oversample = predict(model_lasso_oversample, test_data_scaled_oversample, type = "prob")[, "True"]
predictions_lasso_oversample = ifelse(probabilities_lasso_oversample > 0.5, "True", "False")
confusion_matrix_lasso_oversample = table(predictions_lasso_oversample, test_data_scaled_oversample$Churn)

```


#### Ridge Regression

Ridge regression is another linear regression technique that includes a penalty term to reduce model complexity and prevent overfitting. Unlike Lasso regression, which can shrink some coefficients to zero and perform feature selection, with Ridge the coefficients tend to zero but they are never exactly to zero. 

```{r}

set.seed(1)
ridge_fit_oversample = glmnet(x = X_train_oversample,
                              y = Y_train_oversample,
                              alpha = 0)
```

```{r echo=FALSE}
plot(ridge_fit_oversample,
     label = F, 
     xvar = "lambda")
legend("topright", legend = colnames(X_train_oversample), 
       col = 1:ncol(X_train_oversample), lty = 1, cex = 0.5)

```
The plot confirms the earlier statement: as the parameter Lambda increases, the coefficients shrink towards zero but never actually reach it, in contrast with what we have previously shown with Lasso.
Another relevant observation is that the plot highlights the most important features, namely International Plan, Total daily minutes, Customer service calls, and Total night minutes. These features are shrunk at the lowest rate, indicating their significance in the model

We then proceed, as we have done for 

```{r}

set.seed(1)
model_ridge_oversample = train(Churn ~ ., 
                               data = train_data_scaled_oversample, 
                               method = "glmnet", 
                               metric = "Accuracy", 
                               trControl = trainControl(method = "cv", number = 10), 
                               tuneGrid = expand.grid(alpha = 0, lambda = seq(0, 0.15, length = 30)))


```

```{r echo=FALSE}


model_ridge_oversample %>% 
  ggplot(aes(x = lambda, y = Accuracy)) + 
  geom_line() + 
  geom_point() +
  geom_text(aes(label = sprintf("%.3f", Accuracy)), check_overlap = TRUE, vjust = -0.5, size = 2.5) + 
  labs(x = TeX("Lambda ($\\lambda$)"), y = "Accuracy", title = "Accuracy vs. Lambda for Ridge Regularization") +
  theme_minimal() + 
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold")
  )


cat("The highest accuracy is:", max(model_ridge_oversample$results$Accuracy),
    "with parameter lambda =", model_ridge_oversample$bestTune$lambda)


```
```{r include=FALSE}

probabilities_ridge_oversample = predict(model_ridge_oversample, test_data_scaled_oversample, 
                                         type = "prob")[,"True"]
predictions_ridge_oversample = ifelse(probabilities_ridge_oversample > 0.5, "True", "False")
confusion_matrix_ridge_oversample = table(predictions_ridge_oversample, test_data_scaled_oversample$Churn)

```

### Comparisson between penalized approaches

```{r echo=FALSE}

## Confusion matrices

# Create the plots
lasso_plot = plot_confusion_matrix(confusion_matrix_lasso_oversample, "Lasso")
ridge_plot = plot_confusion_matrix(confusion_matrix_ridge_oversample, "Ridge")


# Arrange the plots in a single layout
grid.arrange(lasso_plot, ridge_plot, ncol = 2, top = textGrob("Confusion Matrices", gp = gpar(fontsize = 20, fontface = "bold")))


```

```{r echo=FALSE, warning=FALSE}

## ROC

# Compute ROC curves

library(pROC)
par(mfrow=c(1,2)) # 3 rows, 1 column layout for confusion matrices
roc_lasso = roc(test_data$Churn, probabilities_lasso_oversample, 
             plot = TRUE, main = "Lasso", col = "blue", lwd = 3, 
             auc.polygon = TRUE, print.auc = TRUE)
roc_aic = roc(test_data$Churn, 
              probabilities_ridge_oversample, 
              plot = TRUE, main = "Ridge", col = "red", lwd = 3, 
              auc.polygon = TRUE, print.auc = TRUE)
par(mfrow=c(1,1))


```




### Non linear methods

#### Random Forests

With Random Forests, a number of decision trees is built on bootstrapped training samples. However, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of predictors.

We first start by building a simple model with default parameters. The number of predictors considered at each split (mtry in the code) is typically the square root of the number of the predictors. In our case is then 4. We also consider a forest with 500 trees. 
```{r}

set.seed(1)
random_forest_model_oversample = randomForest(Churn ~ ., data = train_data_oversample, 
                                              mtry = 4, 
                                              ntree = 500, 
                                              importance = T)
```

In this first analysis, we are not really interested on the model performance but we just want to know how the OOB error changes increasing the number of the trees.

```{r}

# Storing the error rate matrix
error_rate_matrix_oversample = random_forest_model_oversample$err.rate

# Creating the error rate data frame for plotting
error_rate_matrix_oversample = data.frame(
  Trees = rep(1:nrow(error_rate_matrix_oversample), times = 3),
  Type = rep(c("OOB", "False", "True"), each = nrow(error_rate_matrix_oversample)),
  Error = c(error_rate_matrix_oversample[, "OOB"],
            error_rate_matrix_oversample[, "False"],
            error_rate_matrix_oversample[, "True"])
)

# Plotting the error rate
ggplot(data = error_rate_matrix_oversample, aes(x = Trees, y = Error)) +
  geom_line(aes(color = Type))

```
We can see that the out-of-bag (OOB) error stabilizes after around 100 trees, indicating that the random forest model achieves consistent performance. The error for the "True" class is significantly lower than the "False" class, suggesting the model is better at predicting "True" than "False".

We can also show the features importance considering the metric of mean decrease accuracy.

```{r echo=FALSE}

importance_values = importance(random_forest_model_oversample)

# Plotting the feature importance plot for MeanDecreaseAccuracy 
importance_df = data.frame(
  Feature = rownames(importance_values),
  MeanDecreaseAccuracy = importance_values[, "MeanDecreaseAccuracy"]
)

importance_df = importance_df[order(importance_df$MeanDecreaseAccuracy, decreasing = TRUE), ]
par(mar = c(5, 8, 4, 2) + 0.1)  
barplot(
  importance_df$MeanDecreaseAccuracy,
  names.arg = importance_df$Feature,
  las = 2,
  col = "skyblue",
  main = "Feature Importance - Mean Decrease in Accuracy",
  xlab = "Mean Decrease in Accuracy",
  cex.names = 0.7, 
  horiz = TRUE
)
par(mar = c(5, 4, 4, 2) + 0.1) 

```
We can see that "Total day minutes", "Customer service calls", "Total eve minutes" and "International Plan" are among the most important features, as they have the highest values in both metrics, indicating their strong influence on model accuracy and confirming our findings so far.


One of the most straighforward drawback of random forests is their tendency to overfit the data. This is caused by the fact that the algorithm is based on the construction of decision trees which are particularly prone to overfitting. Even if random forests address the issue increasing the variability of the model, a process of hyperparameter tuning is fundamental for an optimal performance. 

In our R reference code, we adapted two different strategies in order to tune the parameters. In the first one, we used a classic approach with the function "train", specifying a tune grid to test with cross validation (5 folds). However, this function allows to automatically tune only the parameter mtry, and hence it does not consider other relevant parameters such as the number of trees, the maximum number of nodes and the node size. Hence, we decided to build a grid, randomly sample combinations and test them considering the OOB error, an accurate way to estimate the test error. Since the latter yelds better results, we decided to report it.

```{r include=FALSE}

# Create a random sample of hyperparameter combinations
set.seed(1)
hyperparameter_grid <- expand.grid(
  n_tree = c(400,450,500,550,600),
  m_try = 1:12,
  max_nodes = c(10, 25, 50, 75, 100, 200, 300),
  node_size = 1:10
)

# Sample a smaller grid for random search
sampled_grid = hyperparameter_grid %>% sample_n(50)

# Evaluate all combinations
results_random_forest_tuning = data.frame()
for (i in 1:nrow(sampled_grid)) {
  params = sampled_grid[i, ]
  print(params)
  rf_model = randomForest(
    Churn ~ .,
    data = train_data_oversample,
    ntree = params$n_tree,
    mtry = params$m_try,
    maxnodes = params$max_nodes,
    nodesize = params$node_size,
    importance = TRUE
  )
  # Calculate the mean error rate
  mean_err_rate = mean(rf_model$err.rate[, 1])
  print(mean_err_rate)
  
  # Appending the results in the dataframe
  results_random_forest_tuning = rbind(results_random_forest_tuning,
                                       cbind(params, mean_err_rate))
}

```

```{r echo=FALSE}

# Find the best parameters
best_parameters_random_forest = results_random_forest_tuning %>%
  filter(mean_err_rate == min(mean_err_rate))
library(knitr)

kable(best_parameters_random_forest, caption = "Best Parameters for Random Forest")

```

Once we have found our tuned hyperparameters, we use them in order to build the model that will be used in order for us to predict Customer Churn.

```{r}

# Train the final model with the best parameters
final_rf_model <- randomForest(
  Churn ~ .,
  data = train_data_oversample,
  ntree = best_parameters_random_forest$n_tree,
  mtry = best_parameters_random_forest$m_try,
  nodesize = best_parameters_random_forest$node_size,
  maxnodes = best_parameters_random_forest$max_nodes,
  importance = TRUE
)

```

```{r include=FALSE}

# Make predictions on the test set
probabilities_rf = predict(final_rf_model, test_data, type = "prob")[, "True"]
predictions_rf = ifelse(probabilities_rf > 0.5, "True", "False")
confusion_matrix_rf = table(predictions_rf, test_data$Churn)

```

### XGBOOST

XGBoosting is an advanced implementation of gradient boosting that builds an ensemble of decision trees sequentially, with each tree correcting the errors of the previous ones, optimized for speed and performance.

Since XGBoost requires numerical input and cannot directly handle categorical variables, a first essential step is to create dummy variables.

```{r warning=FALSE}

# Create dummy variables for the data
X_train_oversample_dummy = dummyVars(Churn ~ ., data = train_data_oversample) %>% predict(train_data_oversample)
Y_train_oversample_dummy = train_data_oversample$Churn %>% as.numeric(.) - 1
X_test_oversample_dummy = dummyVars(Churn ~ ., data = test_data) %>% predict(test_data)
Y_test_oversample_dummy = test_data$Churn %>% as.numeric(.) - 1

```

In order to have a general understanding of the performance of the model, we firstly train it without tuning.

```{r message=FALSE, results='hide'}

# In the first place we train the algorithm without tuning
set.seed(1)
model_xg = xgboost(as.matrix(X_train_oversample_dummy), 
                   label = Y_train_oversample_dummy, 
                 nrounds = 50, 
                 objective = "binary:logistic", 
                 eval_metric = "error")


```

To gain insight into the relevance of the ntreelimit parameter (and thereby the importance of tuning hyperparameters), we consider the error on the test set. It is important to note that this analysis is not part of the model training process but is merely an evaluation of the parameter, as models should not be trained on the test set.

```{r echo=FALSE}

train_errors = model_xg$evaluation_log$train_error
val_errors = numeric(50)

for (j in 1:50) {
  pred_j = ifelse(predict(model_xg, X_test_oversample_dummy)> 0.5, 1, 0)
  val_errors[j] <- mean(pred_j != Y_test_oversample_dummy)
}

# Plot the error rates
plot(1:50, val_errors, type = "b", xlab = "Number of trees", ylab = "Error", col = 3, ylim = c(0, 0.3), cex = 0.5)
points(1:50, train_errors, type = "b", cex = 0.5)
legend("topright", legend = c("Train", "Test"), col = c(1, 3), lty = 1, lwd = 2, cex = 0.7)

```
```{r include=FALSE}

# We then consider its performance
probabilities_xg = predict(model_xg, X_test_oversample_dummy)
predictions_xg = ifelse(probabilities_xg> 0.5, 1, 0)
confusion_matrix_xg = table(predictions_xg, test_data$Churn)

```

The plot shows that as the number of trees increases, the training error consistently decreases, approaching zero, indicating that the model fits the training data very well. However, the test error stabilizes at a low value early on and remains relatively constant, suggesting that additional trees do not significantly improve the model's performance on the test data. This indicates that the model generalizes well without overfitting, as there is a minimal gap between the training and test errors after a certain number of trees.

We then proceed with the process of hyperparameter tuning using cross validation. 
Since tasks like cross-validation can be paralized, in order to speed up computations, we created a cluster of 5 worker nodes with the library doParallel.

```{r}

cl = makePSOCKcluster(5)
registerDoParallel(cl)

```


```{r include=FALSE}

tune_grid <- expand.grid(
  nrounds = c(10, 20, 30, 40, 50, 60, 70, 80, 90, 100),
  eta = 0.3,
  max_depth = 5,
  subsample = 1,
  colsample_bytree = 1,
  min_child_weight = 5,
  gamma = c(0.1, 0.2, 0.5, 0.75, 1)
)

fitControl <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10,
  search = "random"
)

```

```{r warning=FALSE}

set.seed(1)
model_xg_cv <- train(
  Churn ~ ., data = train_data_oversample, 
  method = "xgbTree", 
  trControl = fitControl,
  verbose = FALSE, 
  tuneGrid = tune_grid,
  objective = "binary:logistic", 
  eval_metric = "error"
)

```

```{r echo=FALSE, warning=FALSE}

# Stop parallel processing
stopCluster(cl)

# Plot the cross-validation results
trellis.par.set(caretTheme())
plot(model_xg_cv)

```

The plot shows the accuracy of an XGBoost model (evaluated via repeated cross-validation) across different boosting iterations (# Boosting Iterations) and minimum loss reduction (gamma) values. The number of boosting iterations significantly impacts the model's accuracy up to a certain point, after which the benefit plateaus. The gamma parameter (minimum loss reduction) has little to no effect on the model's performance within the tested range, indicating that the model is relatively insensitive to this hyperparameter for the given dataset.

```{r include=FALSE}

# Predictions on the test set using the best model
probabilities_xg_cv = predict(model_xg_cv, test_data, type = "prob")[,"True"]
predictions_xg_cv = ifelse(probabilities_xg_cv > 0.5, "True", "False")
confusion_matrix_xg_cv = table(predictions_xg_cv, test_data$Churn)

```


#### Comparisson between Tree based models

As we have done before, to firstly asses the performances we plot the confusion matrices for Random Forests and XGBOOST (with and without tuning).

```{r echo=FALSE}

## Confusion matrices

# Create the plots
rf_plot = plot_confusion_matrix(confusion_matrix_rf, "Random Forest")
xg_plot = plot_confusion_matrix(confusion_matrix_xg, "XGBOOST")
xg_cv_plot = plot_confusion_matrix(confusion_matrix_xg_cv, "XGBOOST (Tuned)")

# Arrange the plots in a single layout
grid.arrange(rf_plot, xg_plot, xg_cv_plot, ncol = 3, top = textGrob("Confusion Matrices", gp = gpar(fontsize = 20, fontface = "bold")))

```



```{r echo=FALSE, warning=FALSE, message=FALSE}

## ROC

# Compute ROC curves

library(pROC)
par(mfrow=c(1,3)) # 3 rows, 1 column layout for confusion matrices
roc_rf = roc(test_data$Churn, probabilities_rf, 
                   plot = TRUE, main = "Random Forest", col = "blue", lwd = 3, 
                   auc.polygon = TRUE, print.auc = TRUE)
roc_aic = roc(test_data$Churn, 
              probabilities_xg, 
              plot = TRUE, main = "XGBOOST", col = "red", lwd = 3, 
              auc.polygon = TRUE, print.auc = TRUE)
roc_bic = roc(test_data$Churn, 
              probabilities_xg_cv, 
              plot = TRUE, main = "XGBOOST (Tuned)", col = "purple", lwd = 3, 
              auc.polygon = TRUE, print.auc = TRUE)
par(mfrow=c(1,1))


```

## CONCLUSIONS

Given the high imbalance in our dataset, we’ve prioritized sensitivity in our selection criteria because minimizing false negatives—thereby reducing missed positives—is crucial for our analysis. In the telecom industry, missing a false negative (failing to predict a customer’s churn) is more costly than identifying a false positive (predicting a customer will churn when they won’t). This approach helps ensure we capture as many potential churners as possible, enabling proactive retention efforts and ultimately reducing customer attrition. Balancing sensitivity with accuracy remains important, but sensitivity is our primary focus to mitigate the higher cost of missed churn predictions.

```{r echo=FALSE}
# Calculate metrics for each model
metrics_baseline = get.metrics(confusion_matrix_baseline_logistic_oversample)
metrics_aic = get.metrics(confusion_matrix_aic_oversample)
metrics_bic = get.metrics(confusion_matrix_bic_oversample)

# Combine metrics into a single data frame
metrics_combined = rbind(metrics_baseline, metrics_aic, metrics_bic)
rownames(metrics_combined) = c("Baseline", "AIC", "BIC")

# Round the metrics to 2 decimal places
metrics_combined = round(metrics_combined, 4)

# Create the table plot
grid.newpage()
grid.draw(plot_table(metrics_combined))

```


```{r echo=FALSE}

## Metrics

metrics_rf = get.metrics(confusion_matrix_rf)
metrics_xg = get.metrics(confusion_matrix_xg)
metrics_xg_cv = get.metrics(confusion_matrix_xg_cv)

# Combine metrics into a single data frame
metrics_combined = rbind(metrics_rf, metrics_xg, metrics_xg_cv)
rownames(metrics_combined) = c("Random Forest", "XGBOOST", "XGBOOST (Tuned)")

# Round the metrics to 2 decimal places
metrics_combined = round(metrics_combined, 4)

# Create the table plot
grid.newpage()
grid.draw(plot_table(metrics_combined))

```

```{r echo=FALSE}

## Metrics

metrics_lasso = get.metrics(confusion_matrix_lasso_oversample)
metrics_ridge = get.metrics(confusion_matrix_ridge_oversample)


# Combine metrics into a single data frame
metrics_combined = rbind(metrics_lasso, metrics_ridge)
rownames(metrics_combined) = c("Lasso", "Ridge")

# Round the metrics to 2 decimal places
metrics_combined = round(metrics_combined, 4)

# Create the table plot
grid.newpage()
grid.draw(plot_table(metrics_combined))

```
## Task 2

For this task, we provided a customer segmentation which is based on the CLV
index (Customer Lifetime Value) and provides insights about groups of customers
that the company can target with ad-hoc promotional campains in response to
possible churns. In this way the company would be more efficient in targeting
only customers who have a large CLV.

## Loading Libraries(TO BE REMOVED)

```{r}
library(ggplot2)
library(dplyr)
library(skimr)
library(readr)
library(sf)
library(usmap)
library(grid)
library(gridExtra)
library(corrplot)
library(caret)
library(ROSE)
library(factoextra)
library(cluster)
library(ggsci)
```

## Importing Dataset(TO BE REMOVED)

```{r}
Data <- read.csv2("path/to/TelecomChurn.csv", 
                 header = TRUE, 
                 sep = ",", 
                 colClasses = "character")
variables <- colnames(Data)
categorical_variables <- c("State", "International.plan", "Voice.mail.plan", "Area.code")
target_variable <- "Churn"
numerical_variables <- setdiff(variables, c(categorical_variables, target_variable))
```

## Data Preprocessing(TO BE REMOVED)

```{r}
# Ensuring that variables are converted in the correct form
Data[[target_variable]] <- as.factor(Data[[target_variable]])
for (var in numerical_variables) {
  Data[[var]] <- as.numeric(Data[[var]])
}
for (var in categorical_variables) {
  Data[[var]] <- as.factor(Data[[var]])
}

# Dropping the variable used in the for loop
rm(var)
```

## Analysis

We begin by computing some indeces that we will need for the analysis. We are
basically creating other features.

### Calculating ARPU (Average Revenue Per User)

The ARPU (Average Revenue Per User) is an important index that we'll use later
to compute the CLV. This index groups all the charges for the day, eve and night.

```{r}
Data <- Data %>%
  mutate(TotalCharges = Total.day.charge + Total.eve.charge + Total.night.charge + Total.intl.charge)
ARPU <- mean(Data$TotalCharges)
ARPU
```

### Calculating Total Daily Minutes

This is another "summary" of the time spent by the customer using the service.
The creation of this index is for the sole purpose of enlarging the set of 
features that we'll use for clustering.

```{r}
Data <- Data %>%
  mutate(Total.minutes = Total.day.minutes + Total.eve.minutes + Total.night.minutes)
```

### Customer Retention Rate
Calculate Customer Retention Rate. Namely, the proportion of customers who did
not churned.

```{r}
retention_rate <- 1 - mean(Data$Churn == "True")
retention_rate
```

### Average Customer Lifespan
Estimate Average Customer Lifespan based on the assumption that if a fraction of
customers churn each period, then on average, a customer will stay for 1/churn 
rate periods. Lower "Churn Rate" means higher "Customer Lifespan".

```{r}
average_lifespan <- 1 / (1 - retention_rate)
average_lifespan
```

### Customer Lifetime Value (CLV)

```{r}
CLV <- ARPU * average_lifespan
CLV
```

```{r}
Data <- Data %>%
  mutate(CLV = TotalCharges * avg_customer_lifespan)

```

Here we add to the Data the column "CLV".

## Visualization

Now that we have computed the CLV index, which is basically a summary of the
various charges paid by each customer, we analyze it like we did for the
other features in the EDA to gain insight about its distribution and its
relation with some other features.

### Distribution of CLV

```{r}
ggplot(Data, aes(x = CLV)) +
  geom_histogram(binwidth = 50, fill = "#0073C2FF", color = "black", alpha = 0.7) +
  theme_minimal(base_size = 15) +
  labs(
    title = "Distribution of Customer Lifetime Value (CLV)",
    x = "Customer Lifetime Value (CLV)",
    y = "Frequency"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold")
  )
```

The distribution of the CLV seems to be normal (a little bit skewed to the
right) the highest value is around 400,00.

### CLV by Top Ten States

```{r}
# Identify the top ten most common states
top_states <- Data %>%
  count(State, sort = TRUE) %>%
  top_n(10, wt = n) %>%
  pull(State)

# Filter the data to include only the top ten states
filtered_data <- Data %>% filter(State %in% top_states)

```

```{r}
# CLV by State for Top Ten States
ggplot(filtered_data, aes(x = State, y = CLV)) +
  geom_boxplot(fill = "#0073C2FF", alpha = 0.7, outlier.color = "red", outlier.shape = 16) +
  theme_minimal(base_size = 15) +
  labs(
    title = "CLV by State (Top Ten States)",
    x = "State",
    y = "Customer Lifetime Value (CLV)"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```

It seemed interesting to us gaining insight also about the distribution of CLV 
by the top ten states.

### CLV by International Plan

```{r}
# CLV by International Plan
ggplot(Data, aes(x = International.plan, y = CLV)) +
  geom_boxplot(fill = "#0073C2FF", alpha = 0.7, outlier.color = "red", outlier.shape = 16) +
  theme_minimal(base_size = 15) +
  labs(
    title = "CLV by International Plan",
    x = "International Plan",
    y = "Customer Lifetime Value (CLV)"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold")
  )
```

### Clustering

In this section we build up the clustering algorithms. We are going to try two 
clustering techniques: K-means and Hierarchical clustering. In addition we are
using the Elbow method to identify the optimal number of clusters (k). At the
end, we are going to choose only one method to go on with.

## Clustering Features

Selecting the features with which carry on the analysis. As said before, those 
three variables are the ones that are more representative and useful for our
purpose.

```{r}
# Select features for clustering
features <- Data %>%
  select(Account.length, Total.minutes, CLV)

# Normalize the features
features_scaled <- scale(features)
```

### K-Means

## Elbow Method K-Means

```{r}
# Determine the optimal number of clusters using the Elbow method
set.seed(1)
wss <- sapply(1:10, function(k) {
  kmeans(features_scaled, centers = k, nstart = 10)$tot.withinss
})

# Print the optimal number of clusters by identifying the elbow point
optimal_k <- which(diff(diff(wss)) == min(diff(diff(wss)))) + 1

# Create a data frame for plotting
df <- data.frame(Clusters = 1:10, WSS = wss)

# Function to plot the Elbow method
plot_elbow_method <- function(df, optimal_k) {
  ggplot(df, aes(x = Clusters, y = WSS)) +
    geom_line(color = "#0073C2FF", size = 1.2) +
    geom_point(color = "#0073C2FF", size = 3) +
    geom_point(aes(x = optimal_k, y = WSS[optimal_k]), color = "red", size = 4) +
    geom_text(aes(x = optimal_k, y = WSS[optimal_k], label = paste("Optimal k =", optimal_k)), 
              color = "red", size = 5, hjust = -0.3, fontface = "bold") +
    labs(title = "Elbow Method for Determining Optimal Number of Clusters",
         x = "Number of Clusters",
         y = "Total Within-Clusters Sum of Squares") +
    theme_minimal(base_size = 15) +
    theme(
      plot.title = element_text(hjust = 0.5, size = 20, face = "bold", color = "black"),
      axis.title = element_text(size = 16, face = "bold", color = "black"),
      axis.text = element_text(size = 14, face = "bold", color = "black"),
      panel.grid.major = element_line(color = "grey80"),
      panel.grid.minor = element_line(color = "grey90"),
      panel.background = element_rect(fill = "whitesmoke", color = NA)
    )
}

# Call the function to plot the Elbow method
plot_elbow_method(df, optimal_k)
```

We found out that the optimal number of clusters is 9. 

## K-Means in action

```{r}
# Apply K-means clustering with the chosen number of clusters (k = 8)
set.seed(1)
kmeans_result <- kmeans(features_scaled, centers = 9, nstart = 25)

# Append cluster results to the original data
Data$Cluster <- as.factor(kmeans_result$cluster)

# Visualize the clusters
fviz_cluster(kmeans_result, data = features_scaled, geom = "point", ellipse.type = "convex", palette = "jco", ggtheme = theme_minimal())

```

### Hierarchical Clustering

Now it's the time of hierarchical.

## Elbow method Hierarchical clustering

```{r}
# Compute the Distance Matrix
distance_matrix <- dist(features_scaled)

# Perform Hierarchical Clustering
hc_result <- hclust(distance_matrix, method = "ward.D2")

# Calculate the total within-cluster sum of square (wss) for different numbers of clusters
set.seed(1)
wss <- function(Data, max_clusters) {
  wss_values <- numeric(max_clusters)
  for (k in 1:max_clusters) {
    hc <- hclust(dist(Data), method = "ward.D2")
    cluster_assignments <- cutree(hc, k = k)
    wss_values[k] <- sum(sapply(unique(cluster_assignments), function(cluster) {
      cluster_data <- Data[cluster_assignments == cluster, ]
      sum(dist(cluster_data)^2) / nrow(cluster_data)
    }))
  }
  return(wss_values)
}

# Define the maximum number of clusters to consider
max_clusters <- 10

# Compute the wss for each number of clusters
wss_values_h <- wss(features_scaled, max_clusters)

# Plot the Elbow Method
plot(1:max_clusters, wss_values_h, type = "b", pch = 19, frame = FALSE, 
     xlab = "Number of Clusters", ylab = "Within-cluster Sum of Squares", 
     main = "Elbow Method for Finding Optimal k")

# Print the optimal number of clusters by identifying the elbow point
optimal_kh <- which(diff(diff(wss_values_h)) == min(diff(diff(wss_values_h)))) + 1

points(optimal_kh, wss_values_h[optimal_kh], col = "red", pch = 19)
text(optimal_kh, wss_values_h[optimal_kh], labels = paste("Elbow at k =", optimal_kh), pos = 4, col = "red")
```

Surprisingly we found that also for hierarchical clustering we have an optimal k
equal to 9.

## Hierarchical clustering in action

```{r}

# Visualize the dendrogram
plot(hc_result, cex = 0.5, main = "Dendrogram", xlab = "", sub = "", labels = FALSE)
rect.hclust(hc_result, k = 9, border = 2:5)

cutree_result_h <- cutree(hc_result, k = 9)

# Add the cluster labels to the original data
Data$Cluster_h <- as.factor(cutree_result_h)

fviz_cluster(list(data = features_scaled, cluster = cutree_result),
             geom = "point",
             ellipse.type = "convex",
             palette = "jco",
             ggtheme = theme_minimal(),
             main = "Cluster Visualization using Hierarchical Clustering")


```

## Digression on the two methods

We ended up choosing the K-means since it represents in a clearer way the
clusters without "overlapping" (problem that we encounter adopting the 
hierarchical method).

## Analysis of the Clusters

```{r}
# Analyze cluster characteristics
cluster_summary <- Data %>%
  group_by(Cluster) %>%
  summarize(across(c(Account.length, Total.minutes, CLV), mean, .names = "mean_{.col}")) %>%
  arrange(Cluster)

# Reshape the data for plotting
cluster_summary_long <- cluster_summary %>%
  pivot_longer(cols = starts_with("mean_"), names_to = "Metric", values_to = "MeanValue") %>%
  mutate(Metric = factor(Metric, levels = c("mean_Account.length", "mean_Total.minutes", "mean_CLV"),
                         labels = c("Account Length", "Total Minutes", "CLV")))

# Plotting the cluster characteristics
ggplot(cluster_summary_long, aes(x = Cluster, y = MeanValue, fill = Cluster)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ Metric, scales = "free_y") +
  theme_minimal(base_size = 15) +
  labs(
    title = "Cluster Characteristics",
    x = "Cluster",
    y = "Mean Value",
    fill = "Cluster"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold"),
    legend.position = "none"
  )


# Analyze the number of churned customers in each cluster
churn_analysis <- Data %>%
  group_by(Cluster, Churn) %>%
  summarise(Count = n()) %>%
  arrange(Cluster, Churn)

ggplot(churn_analysis, aes(x = Cluster, y = Count, fill = Churn)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme_minimal(base_size = 15) +
  labs(
    title = "Number of Churned Customers in Each Cluster",
    x = "Cluster",
    y = "Number of Customers",
    fill = "Churn Status"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold"),
    legend.title = element_text(face = "bold")
  )


# Analyze the Avg. CLV values in each cluster
CLV_analysis <- Data %>%
  group_by(Cluster) %>%
  summarise(Max_CLV = max(CLV)) %>%
  arrange(Cluster)

ggplot(CLV_analysis, aes(x = Cluster, y = Max_CLV, fill = Cluster)) +
  geom_bar(stat = "identity") +
  theme_minimal(base_size = 15) +
  labs(
    title = "Maximum Customer Lifetime Value (CLV) in Each Cluster",
    x = "Cluster",
    y = "Maximum CLV",
    fill = "Cluster"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold"),
    legend.position = "none"
  )

```

We can see from the last two analyses that at the end 9 clusters came out. We
also gained insight about the highest value of CLV in each cluster notice that
the 7th cluster has the highest value overall and the number of customers
that churned is bigger in the 7th cluster. This leads to the acknowledgement
of the fact that the firm must adopt a certain treatment mainly to the 7th
cluster of customers.

## Conclusion

This analysis highlights key insights into customer churn and CLV in the telecom 
sector. Specific attention should be given to clusters with high churn rates and 
high CLV for targeted retention strategies. In this way the company would save 
money and effort, optimizing in this way promotional strategies for customer 
retention.


